# 激活函数和损失函数

## 输出层激活函数和损失函数

输出层的**激活函数**和**损失函数**由任务类型决定，它们与隐藏层激活函数的选择是独立的，一般选择机制如下：  

![image](https://pic4.zhimg.com/v2-0a7016d069c25a1aa65384349d9d24ee_r.jpg)  

二分类时应用sigmoid函数，是softmax函数在二分类时的特殊情况。

## 交叉熵损失函数为什么有两类？？

假设我们现在有一个样本$ \{x,t\} $，这两种交叉熵损失函数分别是：  

- `$-t_jlog(y_j)$`，`${t_j}$`说明样本是第`$j$`类
- `$-\sum_{i}t_ilog(y_i)+(1-t_i)log(1-y_i)$`

这两个交叉熵损失函数不同，主要是输出层的激活函数不同。
首先看信息论中交叉熵的定义：
```math
-\int p(x)\log g(x)
```
交叉熵描述两个分布的距离，神经网络训练的目的是`$g(x)$`逼近`$p(x)$`。
- 多分类时，输出层激活函数为softmax函数，`$g(x)$`就是最后一层的输出`$y$`。`$p(x)$`就是`$one-hot$`标签。带入到交叉熵定义中，就得到第一个式子
- 二分类时，输出层激活函数为sigmoid函数，因为加起来不为1，所以不能当作一个分布来看待了，应该把每个神经元都当作一个分布，第`$i$`个神经元交叉熵为：
```math
-t_ilog(y_i)+(1-t_i)log(1-y_i)
```
所以最后一层总的交叉熵损失函数为

```math
-\sum_{i}t_ilog(y_i)+(1-t_i)log(1-y_i)
```
